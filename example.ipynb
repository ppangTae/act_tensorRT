{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c22235d5",
   "metadata": {},
   "source": [
    "\n",
    "polygraphy를 설치하기 위해서는 다음의 명령어를 사용해야한다.\n",
    "\n",
    "`python -m pip install colored polygraphy --extra-index-url https://pypi.ngc.nvidia.com`\n",
    "\n",
    "polygraphy는 의존하는 패키지가 많이 없기때문에 아래에 `AUTOINSTALL_DEPS`를 True로 바꿔주면 알아서 설치가된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3071f456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "polygraphy.config.AUTOINSTALL_DEPS=True\n"
     ]
    }
   ],
   "source": [
    "import polygraphy\n",
    "polygraphy.config.AUTOINSTALL_DEPS = True\n",
    "polygraphy.config.ASK_BEFORE_INSTALL = True # 지 멋대로 설치되는 것을 방지하기 위해서\n",
    "print(f\"{polygraphy.config.AUTOINSTALL_DEPS=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb9bc0b",
   "metadata": {},
   "source": [
    "# Backends"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144318ee",
   "metadata": {},
   "source": [
    "`Bankends`는 deep learning framework와의 interface를 제공한다. Backends는 Loader와 Runner로 구성되어있다.\n",
    "\n",
    "일단 Loader에 대해서 먼저 알아보자.\n",
    "Polygraphy는 두 가지 Loader를 제공한다. `PascalCase`의 경우 TensorRT엔진을 만들 수 있는 callable을 반환하고, `snakecase`의 경우 callable이 아니라 engine을 반환한다.\n",
    "즉 전자를 사용하면 나중에 호출할 때 engine이 생성되고, 후자를 사용하면 바로 engine이 생성된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca95420d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I] TF32 is disabled by default. Turn on TF32 for better performance with minor accuracy differences.\n",
      "<class 'polygraphy.backend.trt.loader.EngineFromNetwork'>\n"
     ]
    }
   ],
   "source": [
    "from polygraphy.backend.trt import EngineFromNetwork, NetworkFromOnnxPath\n",
    "\n",
    "build_engine = EngineFromNetwork(NetworkFromOnnxPath(\"/home/park/ros2/tensorRT/policy.onnx\"))\n",
    "print(type(build_engine))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "337e9816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I] TF32 is disabled by default. Turn on TF32 for better performance with minor accuracy differences.\n",
      "[I] Configuring with profiles:[\n",
      "        Profile 0:\n",
      "            {qpos [min=[1, 6], opt=[1, 6], max=[1, 6]],\n",
      "             image [min=[1, 3, 3, 480, 640], opt=[1, 3, 3, 480, 640], max=[1, 3, 3, 480, 640]]}\n",
      "    ]\n",
      "\u001b[38;5;11m[W] profileSharing0806 is on by default in TensorRT 10.0. This flag is deprecated and has no effect.\u001b[0m\n",
      "\u001b[38;5;14m[I] Building engine with configuration:\n",
      "    Flags                  | []\n",
      "    Engine Capability      | EngineCapability.STANDARD\n",
      "    Memory Pools           | [WORKSPACE: 11783.31 MiB, TACTIC_DRAM: 11783.31 MiB, TACTIC_SHARED_MEMORY: 1024.00 MiB]\n",
      "    Tactic Sources         | [EDGE_MASK_CONVOLUTIONS, JIT_CONVOLUTIONS]\n",
      "    Profiling Verbosity    | ProfilingVerbosity.DETAILED\n",
      "    Preview Features       | [PROFILE_SHARING_0806]\u001b[0m\n",
      "\u001b[38;5;10m[I] Finished engine building in 14.640 seconds\u001b[0m\n",
      "<class 'tensorrt.tensorrt.ICudaEngine'>\n"
     ]
    }
   ],
   "source": [
    "from polygraphy.backend.trt import engine_from_network, network_from_onnx_path\n",
    "\n",
    "engine = engine_from_network(network_from_onnx_path(\"/home/park/ros2/tensorRT/policy.onnx\"))\n",
    "print(type(engine))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d7a19f",
   "metadata": {},
   "source": [
    "# Runner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6c3ad6",
   "metadata": {},
   "source": [
    "Runner는 Loader를 사용해 모델을 로드하고, 추론을 실행하는 객체이다.\n",
    "Runner를 사용하기 위해서는 activate를 해야하는데 한 번 activate하는데 비용이 크므로, 여러번 하지 않는 것이 좋다. 그리고 Context Manager를 사용하는 것을 권장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f56a5243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I] Configuring with profiles:[\n",
      "        Profile 0:\n",
      "            {qpos [min=[1, 6], opt=[1, 6], max=[1, 6]],\n",
      "             image [min=[1, 3, 3, 480, 640], opt=[1, 3, 3, 480, 640], max=[1, 3, 3, 480, 640]]}\n",
      "    ]\n",
      "\u001b[38;5;14m[I] Building engine with configuration:\n",
      "    Flags                  | []\n",
      "    Engine Capability      | EngineCapability.STANDARD\n",
      "    Memory Pools           | [WORKSPACE: 11783.31 MiB, TACTIC_DRAM: 11783.31 MiB, TACTIC_SHARED_MEMORY: 1024.00 MiB]\n",
      "    Tactic Sources         | [EDGE_MASK_CONVOLUTIONS, JIT_CONVOLUTIONS]\n",
      "    Profiling Verbosity    | ProfilingVerbosity.DETAILED\n",
      "    Preview Features       | [PROFILE_SHARING_0806]\u001b[0m\n",
      "\u001b[38;5;10m[I] Finished engine building in 14.286 seconds\u001b[0m\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'input_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpolygraphy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrt\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TrtRunner\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m TrtRunner(build_engine) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[0;32m----> 4\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m runner\u001b[38;5;241m.\u001b[39minfer(feed_dict\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput0\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43minput_data\u001b[49m})\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_data' is not defined"
     ]
    }
   ],
   "source": [
    "from polygraphy.backend.trt import TrtRunner\n",
    "\n",
    "with TrtRunner(build_engine) as runner:\n",
    "    outputs = runner.infer(feed_dict={\"input0\": input_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df81a1e8-517b-4c3d-81e7-ab90d19f3cdc",
   "metadata": {},
   "source": [
    "## ONNX Model Export"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b43dc3-8376-46fe-bb09-2754a07b1190",
   "metadata": {},
   "source": [
    "ONNX Model을 Export하기 위해서는 `torch.onnx.export`함수를 사용해야한다. 이 함수를 사용할 때 dummy input을 넣어줘야하는데 ACT신경망의 경우, 학습과 추론시에 신경망이 다르다. 따라서 추론시에 어떤 신경망을 사용하는지 살펴보고, 어떻게 dummy input을 구성해야하는지 알아보자. 아래 코드에서 policy가 ACT신경망을 말하는데 여기에 입력이 qpos와 curr_image임을 알 수 있다. 아래 코드는 ACT에서 사용된 ACTPolicy이다. `__call__`이 호출되었을 때 actions가 있으면 training을 진행하고, 없으면 Inference만 진행한다.\n",
    "\n",
    "`onnx_transformation.py`에 관련 코드가 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a5e5f4",
   "metadata": {},
   "source": [
    "# Calibrator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eeae91a",
   "metadata": {},
   "source": [
    "Calibrator는 FP32값을 INT8로 양자화할 때 필요한 Scale과 Zero-point값을 계산해주는 클래스이다. \n",
    "Static Calibration의 경우 미리 그 값을 계산하기때문에 전체 학습데이터를 대표하는 `representative dataset`이 필요하다. Calibrator는 데이터를 받아서 신경망을 통과시킨 뒤 신경망의 각 노드들에서 scale과 zero-point를 계산한다. 따라서, ONNX model을 생성할 때 입력값으로 선정한 크기의 데이터를 생성하는 제너레이터를 인자로 넘겨주어야한다.\n",
    "\n",
    "tensorRT에 소개된 예제 ResNet-50같은 경우는 전처리가 크게 필요하지않지만, ACT의 경우 전처리가 꽤 복잡하다.\n",
    "\n",
    "`act/utils.py`에 `EpisodicDataset`을 참고하면 x가지 전처리를 확인할 수 있다. 우리는 입력으로 qpos와 image만을 전달하기때문에 이들의 전처리만 고려하면된다.\n",
    "1. `act/utils.py`에 `get_norm_stats`함수를 사용해 h5py로 표현된 전체 데이터셋의 qpos의 평균과 분산을 계산한다. 그리고 이 값을 통해 qpos를 정규화한다.\n",
    "2. h5py에 저장된 이미지들을 하나의 numpy array로 합치고, (h w c)로 표현된 이미지를 (c h w)형태로 변환해준다.\n",
    "3. 이미지 데이터를 255로 나눠 정규화한다"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0913c9-143d-425e-9071-44b34c0e2324",
   "metadata": {},
   "source": [
    "# IBuilderConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beca32d9",
   "metadata": {},
   "source": [
    "아래와 같이 `IBuilderConfig`를 작성한다. INT8 양자화를 지원하지않는 레어이가 존재하면 FP32로 계산된다. 이때, FP32가 아닌 FP16으로 계산을 원한다면 `fp16=True` 을 설정해주면 된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef05fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each type flag must be set to true.\n",
    "builder_config = poly_trt.create_config(builder=builder,\n",
    "                                        network=network,\n",
    "                                        int8=True,\n",
    "                                        fp16=True,\n",
    "                                        calibrator=calibrator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60ae7ad-959f-424b-a290-8e96be875167",
   "metadata": {},
   "source": [
    "# TensorRT Engine Build and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "825c4158",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'poly_trt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[43mpoly_trt\u001b[49m\u001b[38;5;241m.\u001b[39mengine_from_network(network\u001b[38;5;241m=\u001b[39m(builder, network, parser),\n\u001b[1;32m      2\u001b[0m                                       config\u001b[38;5;241m=\u001b[39mbuilder_config)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# TensorRT engine will be saved to ENGINE_PATH.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m ENGINE_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/park/ros2/tensorRT/act_int8_fp16.engine\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'poly_trt' is not defined"
     ]
    }
   ],
   "source": [
    "engine = poly_trt.engine_from_network(network=(builder, network, parser),\n",
    "                                      config=builder_config)\n",
    "\n",
    "# TensorRT engine will be saved to ENGINE_PATH.\n",
    "ENGINE_PATH = \"/home/park/ros2/tensorRT/act_int8_fp16.engine\"\n",
    "poly_trt.save_engine(engine, ENGINE_PATH)\n",
    "\n",
    "# Load serialized engine using 'open'.\n",
    "engine = poly_trt.engine_from_bytes(open(ENGINE_PATH, \"rb\").read())\n",
    "print(\"engine이 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac6f3fe",
   "metadata": {},
   "source": [
    "이렇게 Engine을 생성하면 아래와 같은 경고문구가 발생합니다. engine은 성공적으로 생성되긴합니다.\n",
    "아래 경고문구들은 tensorRT가 특정 레이어에서 Scale과 zero-point를 계산하지 못해 int8연산을 하지못하므로 fp16이나 fp32로 게산을 수행하겠다는 경고입니다.\n",
    "\n",
    "저 같은 경우 다음과 같은 Warning이 발생했습니다.\n",
    "[W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1182_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
    "[W] Missing scale and zero-point for tensor model/backbones.0/backbones.0.1/Constant_1_output_0_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
    "[W] Missing scale and zero-point for tensor model.transformer.encoder.layers.3.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
    "[W] Missing scale and zero-point for tensor model.transformer.decoder.layers.0.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
    "\n",
    "어디서 scale과 zero-point를 계산못하는지를 살펴보면 아ㄹ의 레이어들에서 못합니다.\n",
    "\n",
    "1. model/backbones.0/backbones.0.1/Constant_1_output_0_output\n",
    "2. model.transformer.encoder.layers.3.norm1.weight_output\n",
    "3. model.transformer.decoder.layers.0.norm2.weight_output\n",
    "4. ONNXTRT_Broadcast_1182\n",
    "\n",
    "netron을 통해서 살펴보면 첫 번째는 position encoding과정에서 레이어이고, 두 번째, 세 번째는 Transformer에 layer normalization입니다. 네 번째는 잘 모르겠습니다.\n",
    "github에 찾아보니 layer normalization의 경우 원래 안되는 거라고해서 일단은 넘어갔습니다.\n",
    "[Q-DQ nodes for int8 LayerNorm #4084](https://github.com/NVIDIA/TensorRT/issues/4084)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f41ced9-2d80-4809-941f-63ac8026e2e8",
   "metadata": {},
   "source": [
    "# Engine 성능 비교"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ce766c-9fe1-4bcb-a88e-f9aeae01d1d4",
   "metadata": {},
   "source": [
    "Engine의 성능을 비교하기 위해 다음 네 가지 타입의 Engine을 생성합니다.\n",
    "1. act pytorch model\n",
    "2. tensorRT fp32 origin model\n",
    "3. INT8 Static Calibration 적용된 엔진\n",
    "4. INT8 Random Calibration 적용된 엔진(Calibration으로 인한 정확도 향상을 알아보기위해)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f2766c5-96c2-4bf7-9d36-36043ac10f19",
   "metadata": {},
   "source": [
    "위 engine들의 성능 비교 코드는 `compare.py`에 있습니다.\n",
    "성능 비교 결과는 아래와 같습니다.\n",
    "inference시에는 KL Divergence 에러를 계산할 수 없으므로 L1 Loss만을 사용해서 정확도를 평가하였습니다.\n",
    "평가데이터는 총 40개를 사용하였습니다.\n",
    "\n",
    "act_pytorch에서 tensorRT엔진을 만드는 것만으로도 추론시간이 약 2배 빨라졌고, INT8 양자화를 통해 L1 Loss가 약간 올라갔으나 약 7배 향상하였습니다."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
