{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c22235d5",
   "metadata": {},
   "source": [
    "\n",
    "polygraphy를 설치하기 위해서는 다음의 명령어를 사용해야한다.\n",
    "\n",
    "`python -m pip install colored polygraphy --extra-index-url https://pypi.ngc.nvidia.com`\n",
    "\n",
    "polygraphy는 의존하는 패키지가 많이 없기때문에 아래에 `AUTOINSTALL_DEPS`를 True로 바꿔주면 알아서 설치가된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3071f456",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "polygraphy.config.AUTOINSTALL_DEPS=True\n"
     ]
    }
   ],
   "source": [
    "import polygraphy\n",
    "polygraphy.config.AUTOINSTALL_DEPS = True\n",
    "polygraphy.config.ASK_BEFORE_INSTALL = True # 지 멋대로 설치되는 것을 방지하기 위해서\n",
    "print(f\"{polygraphy.config.AUTOINSTALL_DEPS=}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb9bc0b",
   "metadata": {},
   "source": [
    "# Backends"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "144318ee",
   "metadata": {},
   "source": [
    "`Bankends`는 deep learning framework와의 interface를 제공한다. Backends는 Loader와 Runner로 구성되어있다.\n",
    "\n",
    "일단 Loader에 대해서 먼저 알아보자.\n",
    "Polygraphy는 두 가지 Loader를 제공한다. `PascalCase`의 경우 TensorRT엔진을 만들 수 있는 callable을 반환하고, `snakecase`의 경우 callable이 아니라 engine을 반환한다.\n",
    "즉 전자를 사용하면 나중에 호출할 때 engine이 생성되고, 후자를 사용하면 바로 engine이 생성된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ca95420d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I] TF32 is disabled by default. Turn on TF32 for better performance with minor accuracy differences.\n",
      "<class 'polygraphy.backend.trt.loader.EngineFromNetwork'>\n"
     ]
    }
   ],
   "source": [
    "from polygraphy.backend.trt import EngineFromNetwork, NetworkFromOnnxPath\n",
    "\n",
    "build_engine = EngineFromNetwork(NetworkFromOnnxPath(\"/home/park/ros2/tensorRT/policy.onnx\"))\n",
    "print(type(build_engine))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "337e9816",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I] TF32 is disabled by default. Turn on TF32 for better performance with minor accuracy differences.\n",
      "[I] Configuring with profiles:[\n",
      "        Profile 0:\n",
      "            {qpos [min=[1, 6], opt=[1, 6], max=[1, 6]],\n",
      "             image [min=[1, 3, 3, 480, 640], opt=[1, 3, 3, 480, 640], max=[1, 3, 3, 480, 640]]}\n",
      "    ]\n",
      "\u001b[38;5;11m[W] profileSharing0806 is on by default in TensorRT 10.0. This flag is deprecated and has no effect.\u001b[0m\n",
      "\u001b[38;5;14m[I] Building engine with configuration:\n",
      "    Flags                  | []\n",
      "    Engine Capability      | EngineCapability.STANDARD\n",
      "    Memory Pools           | [WORKSPACE: 11783.31 MiB, TACTIC_DRAM: 11783.31 MiB, TACTIC_SHARED_MEMORY: 1024.00 MiB]\n",
      "    Tactic Sources         | [EDGE_MASK_CONVOLUTIONS, JIT_CONVOLUTIONS]\n",
      "    Profiling Verbosity    | ProfilingVerbosity.DETAILED\n",
      "    Preview Features       | [PROFILE_SHARING_0806]\u001b[0m\n",
      "\u001b[38;5;10m[I] Finished engine building in 14.640 seconds\u001b[0m\n",
      "<class 'tensorrt.tensorrt.ICudaEngine'>\n"
     ]
    }
   ],
   "source": [
    "from polygraphy.backend.trt import engine_from_network, network_from_onnx_path\n",
    "\n",
    "engine = engine_from_network(network_from_onnx_path(\"/home/park/ros2/tensorRT/policy.onnx\"))\n",
    "print(type(engine))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08d7a19f",
   "metadata": {},
   "source": [
    "# Runner"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6c3ad6",
   "metadata": {},
   "source": [
    "Runner는 Loader를 사용해 모델을 로드하고, 추론을 실행하는 객체이다.\n",
    "Runner를 사용하기 위해서는 activate를 해야하는데 한 번 activate하는데 비용이 크므로, 여러번 하지 않는 것이 좋다. 그리고 Context Manager를 사용하는 것을 권장한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f56a5243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[I] Configuring with profiles:[\n",
      "        Profile 0:\n",
      "            {qpos [min=[1, 6], opt=[1, 6], max=[1, 6]],\n",
      "             image [min=[1, 3, 3, 480, 640], opt=[1, 3, 3, 480, 640], max=[1, 3, 3, 480, 640]]}\n",
      "    ]\n",
      "\u001b[38;5;14m[I] Building engine with configuration:\n",
      "    Flags                  | []\n",
      "    Engine Capability      | EngineCapability.STANDARD\n",
      "    Memory Pools           | [WORKSPACE: 11783.31 MiB, TACTIC_DRAM: 11783.31 MiB, TACTIC_SHARED_MEMORY: 1024.00 MiB]\n",
      "    Tactic Sources         | [EDGE_MASK_CONVOLUTIONS, JIT_CONVOLUTIONS]\n",
      "    Profiling Verbosity    | ProfilingVerbosity.DETAILED\n",
      "    Preview Features       | [PROFILE_SHARING_0806]\u001b[0m\n",
      "\u001b[38;5;10m[I] Finished engine building in 14.286 seconds\u001b[0m\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'input_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpolygraphy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbackend\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrt\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m TrtRunner\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m TrtRunner(build_engine) \u001b[38;5;28;01mas\u001b[39;00m runner:\n\u001b[0;32m----> 4\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m runner\u001b[38;5;241m.\u001b[39minfer(feed_dict\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput0\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43minput_data\u001b[49m})\n",
      "\u001b[0;31mNameError\u001b[0m: name 'input_data' is not defined"
     ]
    }
   ],
   "source": [
    "from polygraphy.backend.trt import TrtRunner\n",
    "\n",
    "with TrtRunner(build_engine) as runner:\n",
    "    outputs = runner.infer(feed_dict={\"input0\": input_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8b76171",
   "metadata": {},
   "source": [
    "# Comparator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df81a1e8-517b-4c3d-81e7-ab90d19f3cdc",
   "metadata": {},
   "source": [
    "## ONNX Model Export"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b43dc3-8376-46fe-bb09-2754a07b1190",
   "metadata": {},
   "source": [
    "ONNX Model을 Export하기 위해서는 `torch.onnx.export`함수를 사용해야한다. 이 함수를 사용할 때 dummy input을 넣어줘야하는데 ACT신경망의 경우, 학습과 추론시에 신경망이 다르다. 따라서 추론시에 어떤 신경망을 사용하는지 살펴보고, 어떻게 dummy input을 구성해야하는지 알아보자. 아래 코드에서 policy가 ACT신경망을 말하는데 여기에 입력이 qpos와 curr_image임을 알 수 있다. 저희 로봇 같은 경우는 6개의 관절이 있으므로 qpos는 shpae이 (6,), 3대의 카메라가 있으므로 shape이 (3,480,640,3)입니다.\n",
    "\n",
    "아래 코드는 ACT에서 사용된 ACTPolicy이다. `__call__`이 호출되었을 때 actions가 있으면 training을 진행하고, 없으면 Inference만 진행한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d611db3-57b2-4d47-8db2-d832914cacbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACTPolicy(nn.Module):\n",
    "    def __init__(self, args_override):\n",
    "        super().__init__()\n",
    "        model, optimizer = build_ACT_model_and_optimizer(args_override)\n",
    "        self.model = model # CVAE decoder\n",
    "        self.optimizer = optimizer\n",
    "        self.kl_weight = args_override['kl_weight']\n",
    "        print(f'KL Weight {self.kl_weight}')\n",
    "\n",
    "    def __call__(self, qpos, image, actions=None, is_pad=None):\n",
    "        env_state = None\n",
    "        normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                         std=[0.229, 0.224, 0.225])\n",
    "        image = normalize(image)\n",
    "        if actions is not None: # training time\n",
    "            actions = actions[:, :self.model.num_queries]\n",
    "            is_pad = is_pad[:, :self.model.num_queries]\n",
    "\n",
    "            a_hat, is_pad_hat, (mu, logvar) = self.model(qpos, image, env_state, actions, is_pad)\n",
    "            total_kld, dim_wise_kld, mean_kld = kl_divergence(mu, logvar)\n",
    "            loss_dict = dict()\n",
    "            all_l1 = F.l1_loss(actions, a_hat, reduction='none')\n",
    "            l1 = (all_l1 * ~is_pad.unsqueeze(-1)).mean()\n",
    "            loss_dict['l1'] = l1\n",
    "            loss_dict['kl'] = total_kld[0]\n",
    "            loss_dict['loss'] = loss_dict['l1'] + loss_dict['kl'] * self.kl_weight\n",
    "            return loss_dict\n",
    "        else: # inference time\n",
    "            a_hat, _, (_, _) = self.model(qpos, image, env_state) # no action, sample from prior\n",
    "            return a_hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fbbe488-ca30-411e-aa33-cd4f8e507484",
   "metadata": {},
   "outputs": [],
   "source": [
    "if config['policy_class'] == \"ACT\":\n",
    "    if t % query_frequency == 0:\n",
    "        all_actions = policy(qpos, curr_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c82bbde2-540e-4a69-8177-70169219ee7e",
   "metadata": {},
   "source": [
    "아래는 제가 작성한 ONNX모델 출력코드입니다. qpos, curr_image의 shape에서 batch size만 추가해서 작성해주면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5488a2c-197d-46b3-8ef9-4fa8bb0ea1b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1\n",
    "state_dim = 14\n",
    "num_cameras = len(camera_names)\n",
    "channels = 3\n",
    "height = 480\n",
    "width = 640\n",
    "\n",
    "dummy_qpos = torch.randn(batch_size, state_dim).cuda()\n",
    "dummy_image = torch.randn(batch_size, num_cameras, channels, height, width).cuda()\n",
    "\n",
    "# ONNX로 export\n",
    "onnx_path = os.path.join(ckpt_dir, \"policy.onnx\")\n",
    "torch.onnx.export(\n",
    "    policy,                                 # 변환할 모델\n",
    "    (dummy_qpos, dummy_image),              # 입력 튜플\n",
    "    onnx_path,                              # 저장 경로\n",
    "    export_params=True,                     # 모델 파라미터 저장\n",
    "    opset_version=17,                       # ONNX opset 버전\n",
    "    do_constant_folding=True,               # 상수 폴딩 최적화\n",
    "    input_names=['qpos', 'image'],          # 입력 이름\n",
    "    output_names=['action'],                # 출력 이름 (모델에 따라 다를 수 있음)\n",
    "    dynamic_axes={                          # 배치 크기 등 동적 처리\n",
    "        'qpos': {0: 'batch_size'},\n",
    "        'image': {0: 'batch_size'}\n",
    "    }\n",
    ")\n",
    "print(f\"ONNX 모델이 {onnx_path}에 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a5e5f4",
   "metadata": {},
   "source": [
    "# Calibrator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eeae91a",
   "metadata": {},
   "source": [
    "Calibrator는 INT8추론을 위해 네트워크를 생성할 때 FP32값을 양자화할 때 사용해야하는 파라미터를 계산해주는 클래스이다.\n",
    "`Calibrator(data_loader, cache=None, BaseClass=None, batch_size=None, quantile=None, regression_cutoff=None, algo=None)[source]`\n",
    "Calibrator에는 `data_loader`라는 iterable 혹은 generator를 인자로 전달해줘야한다. 이 `data_loader`를 만들기 위해서 ACT에서 데이터를 어떻게 전처리하고, 어떤 데이터를 가져오는지를 분석할 필요가 있다.\n",
    "\n",
    "해당 내용은 act/utils.py에 있는 `Episodic dataset`과 `get_norm_stats`에 있다. 아래 코드를 보면, 모든 에피소드의 qpos, qvel, action을 numpy array에 저장한 후에, 평균과 표준편차를 계산한 뒤 이를 반환한다. 이 데이터는 데이터를 정규화할 때 사용된다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35aed830",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_norm_stats(dataset_dir, num_episodes):\n",
    "    all_qpos_data = []\n",
    "    all_action_data = []\n",
    "    for episode_idx in range(num_episodes):\n",
    "        dataset_path = os.path.join(dataset_dir, f'episode_{episode_idx}.hdf5')\n",
    "        with h5py.File(dataset_path, 'r') as root:\n",
    "            qpos   = root['/observations/qpos'][()]\n",
    "            qvel   = root['/observations/qvel'][()]\n",
    "            action = root['/action'][()]\n",
    "        all_qpos_data.append(torch.from_numpy(qpos))\n",
    "        all_action_data.append(torch.from_numpy(action))\n",
    "    all_qpos_data   = torch.stack(all_qpos_data)   # shape: [N, T, qpos_dim]\n",
    "    all_action_data = torch.stack(all_action_data) # shape: [N, T, action_dim]\n",
    "\n",
    "    # (1) action 데이터 정규화 통계\n",
    "    action_mean = all_action_data.mean(dim=[0,1], keepdim=True)\n",
    "    action_std  = all_action_data.std(dim=[0,1], keepdim=True)\n",
    "    action_std  = torch.clip(action_std, 1e-2, np.inf)\n",
    "\n",
    "    # (2) qpos 데이터 정규화 통계\n",
    "    qpos_mean = all_qpos_data.mean(dim=[0,1], keepdim=True)\n",
    "    qpos_std  = all_qpos_data.std(dim=[0,1], keepdim=True)\n",
    "    qpos_std  = torch.clip(qpos_std, 1e-2, np.inf)\n",
    "\n",
    "    stats = {\n",
    "      \"action_mean\":  action_mean.numpy().squeeze(),\n",
    "      \"action_std\":   action_std.numpy().squeeze(),\n",
    "      \"qpos_mean\":    qpos_mean.numpy().squeeze(),\n",
    "      \"qpos_std\":     qpos_std.numpy().squeeze(),\n",
    "      \"example_qpos\": qpos  # 마지막 에피소드의 raw qpos 예시\n",
    "    }\n",
    "    return stats\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0afc7eb2",
   "metadata": {},
   "source": [
    "아래 EpisodicDataset을 보면, `__getitem__`함수에서 에피소드의 특정순간(start_ts)의 `qpos`, `qvel`, `image`들고온 후, start_ts이후에 `action`값을 들고온다.\n",
    "학습 시킬 때, `action sequnce`가 필요하고, 에피소드의 끝일 경우 `action sequnce`의 개수가 부족하기때문에 padding을 수행한 후, `is_pad`변수를 통해 paddding이 되어있는지 여부를 저장한다. 이미지는 `k c h w`형태로 변경하고, image, action , qpos를 정규화한다.\n",
    "\n",
    "정리하자면,\n",
    "1. 특정 index의 `qpos`, `image`, `action`을 저장한다. 이때 `action`은 `qpos`의 index이후의 값을 **전부** 저장한다.\n",
    "2. `action`은 padding을 하고, `is_pad`를 통해 padding 여부를 저장한다.\n",
    "3. `qpos`, `image`, `action`를 정규화한다.\n",
    "\n",
    "ACT신경망은 학습할때와 추론할때 사용되는 신경망이 다르다. 학습할 때는 `qpos`, `image`, `action` 모두 필요하지만, 추론할 때는 `qpos`, `image`만 있으면 된다. 따라서, action은 input으로 넣어주지 않아도 된다.\n",
    "\n",
    "이렇게 Pytorch Custom Dataset을 구현하고나면 데이터를 한 개씩 꺼낼 수 있는 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a0d917",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpisodicDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, episode_ids, dataset_dir, camera_names, norm_stats):\n",
    "        super(EpisodicDataset).__init__()\n",
    "        self.episode_ids = episode_ids\n",
    "        self.dataset_dir = dataset_dir\n",
    "        self.camera_names = camera_names\n",
    "        self.norm_stats = norm_stats\n",
    "        self.is_sim = None\n",
    "        self.__getitem__(0) # initialize self.is_sim\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.episode_ids)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sample_full_episode = False # hardcode\n",
    "\n",
    "        episode_id = self.episode_ids[index]\n",
    "        dataset_path = os.path.join(self.dataset_dir, f'episode_{episode_id}.hdf5')\n",
    "        with h5py.File(dataset_path, 'r') as root:\n",
    "            is_sim = root.attrs['sim']\n",
    "            original_action_shape = root['/action'].shape # shape : (timesteps, 6) -> (20s x 30hz, 6)\n",
    "            episode_len = original_action_shape[0]\n",
    "            if sample_full_episode:\n",
    "                start_ts = 0\n",
    "            else:\n",
    "                start_ts = np.random.choice(episode_len)\n",
    "            # get observation at start_ts only\n",
    "            qpos = root['/observations/qpos'][start_ts] # shape : (qpos,)\n",
    "            qvel = root['/observations/qvel'][start_ts]\n",
    "            image_dict = dict()\n",
    "            for cam_name in self.camera_names:\n",
    "                image_dict[cam_name] = root[f'/observations/images/{cam_name}'][start_ts] # shape : (480, 640, 3)\n",
    "            # get all actions after and including start_ts\n",
    "            if is_sim:\n",
    "                action = root['/action'][start_ts:] # shape : (action_len, 6)\n",
    "                action_len = episode_len - start_ts\n",
    "            else:\n",
    "                action = root['/action'][max(0, start_ts - 1):] # hack, to make timesteps more aligned\n",
    "                action_len = episode_len - max(0, start_ts - 1) # hack, to make timesteps more aligned\n",
    "\n",
    "        self.is_sim = is_sim\n",
    "        padded_action = np.zeros(original_action_shape, dtype=np.float32)\n",
    "        padded_action[:action_len] = action\n",
    "        is_pad = np.zeros(episode_len)\n",
    "        is_pad[action_len:] = 1\n",
    "\n",
    "        # new axis for different cameras\n",
    "        all_cam_images = []\n",
    "        for cam_name in self.camera_names:\n",
    "            all_cam_images.append(image_dict[cam_name])\n",
    "        all_cam_images = np.stack(all_cam_images, axis=0) # shape : (3, 480, 640 ,3)\n",
    "\n",
    "        # construct observations\n",
    "        image_data = torch.from_numpy(all_cam_images)\n",
    "        qpos_data = torch.from_numpy(qpos).float()\n",
    "        action_data = torch.from_numpy(padded_action).float()\n",
    "        is_pad = torch.from_numpy(is_pad).bool()\n",
    "\n",
    "        # channel last\n",
    "        image_data = torch.einsum('k h w c -> k c h w', image_data) # shape : (3, 3, 480, 640)\n",
    "\n",
    "        # normalize image and change dtype to float\n",
    "        image_data = image_data / 255.0\n",
    "        action_data = (action_data - self.norm_stats[\"action_mean\"]) / self.norm_stats[\"action_std\"]\n",
    "        qpos_data = (qpos_data - self.norm_stats[\"qpos_mean\"]) / self.norm_stats[\"qpos_std\"]\n",
    "\n",
    "        return image_data, qpos_data, action_data, is_pad"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4325f5bc-e8b7-4a13-9461-1ba0e48a612c",
   "metadata": {},
   "source": [
    "이렇게 Pytorch Custom Dataset을 구현하고나면 데이터를 한 개씩 꺼낼 수 있는 준비는 마친셈이다. train_dataset, val_dataset을 구축하고, dataloader를 통해 해당 데이터들에 접근이 가능하다. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e855f58-ab5f-4c19-889f-26e7b792143b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import EpisodicDataset, load_data, get_norm_stats\n",
    "\n",
    "dataset_dir = \"/home/park/ros2/tensorRT/meloha_box_picking_data\"\n",
    "camera_names = ['cam_head', 'cam_left_wrist','cam_right_wrist']\n",
    "batch_size_train = 8\n",
    "batch_size_val = 8\n",
    "\n",
    "train_dataloader, val_dataloader, stats, _ = load_data(dataset_dir,\n",
    "                                                        41,\n",
    "                                                        camera_names,\n",
    "                                                        batch_size_train,\n",
    "                                                        batch_size_val\n",
    "                                                        )\n",
    "\n",
    "image_data, qpos_data, action_data, is_pad = next(iter(train_dataloader))\n",
    "print(f\"{qpos_data.shape=}\") # qpos_data.shape=torch.Size([8, 6])\n",
    "print(f\"{action_data.shape=}\") # action_data.shape=torch.Size([8, 600, 6])\n",
    "print(f\"{image_data.shape=}\") # image_data.shape=torch.Size([8, 3, 3, 480, 640])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d30361-8db7-41be-9df1-3aad04bee17f",
   "metadata": {},
   "source": [
    "하지만, [Calibrator](https://docs.nvidia.com/deeplearning/tensorrt/archives/tensorrt-1001/polygraphy/docs/backend/trt/calibrator.html?utm_source=chatgpt.com)를 참고해보면 ONNX모델을 만들 때 처럼 입력 이름과 입력 값을 딕셔너리형태로 전달해줘야한다. 값은 Numpy, Pytorch Tensor, GPU Pointers가 가능하다. 기존 코드에서 사용중인 `EpisodicDataset`은 단순히 shape이 (3, 3, 480, 640)인 image_data, shape이 (6,)인 qpos_data, shape이 (600,6)인 action_data, is_pad를 반환하고 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b0913c9-143d-425e-9071-44b34c0e2324",
   "metadata": {},
   "source": [
    "# IBuilderConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beca32d9",
   "metadata": {},
   "source": [
    "저는 아래와 같이 `IBuilderConfig`를 작성하였습니다. INT8 양자화를 지원하지않는 레어이거 존재하면 FP32로 계산됩니다. 이때, FP16으로 계산을 원한다면 `fp16=True` 매개변수를 설정해주면 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef05fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each type flag must be set to true.\n",
    "builder_config = poly_trt.create_config(builder=builder,\n",
    "                                        network=network,\n",
    "                                        int8=True,\n",
    "                                        fp16=True,\n",
    "                                        calibrator=calibrator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60ae7ad-959f-424b-a290-8e96be875167",
   "metadata": {},
   "source": [
    "# TensorRT Engine Build and Save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "825c4158",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'poly_trt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m engine \u001b[38;5;241m=\u001b[39m \u001b[43mpoly_trt\u001b[49m\u001b[38;5;241m.\u001b[39mengine_from_network(network\u001b[38;5;241m=\u001b[39m(builder, network, parser),\n\u001b[1;32m      2\u001b[0m                                       config\u001b[38;5;241m=\u001b[39mbuilder_config)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# TensorRT engine will be saved to ENGINE_PATH.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m ENGINE_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/home/park/ros2/tensorRT/act_int8_fp16.engine\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'poly_trt' is not defined"
     ]
    }
   ],
   "source": [
    "engine = poly_trt.engine_from_network(network=(builder, network, parser),\n",
    "                                      config=builder_config)\n",
    "\n",
    "# TensorRT engine will be saved to ENGINE_PATH.\n",
    "ENGINE_PATH = \"/home/park/ros2/tensorRT/act_int8_fp16.engine\"\n",
    "poly_trt.save_engine(engine, ENGINE_PATH)\n",
    "\n",
    "# Load serialized engine using 'open'.\n",
    "engine = poly_trt.engine_from_bytes(open(ENGINE_PATH, \"rb\").read())\n",
    "print(\"engine이 저장되었습니다.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac6f3fe",
   "metadata": {},
   "source": [
    "따라서, 엔진을 생성하기 전에 가장 먼저 생성해야하는 클래스이다.\n",
    "엔진은 `network_from_onnx_path`와 `engine_from_network`를 통해 생성이 가능하다. 둘 다 `from polygraphy.backend.trt`안에 들어있는 함수이다.\n",
    "`network_from_onnx_path`는 ONNX모델의 경로를 받아 TensorRT network, builder, parser를 반환한다. (!network, builder, parser가 정확히 뭔지 모르겠다)!\n",
    "\n",
    "builder와 network를 통해 `IBuilderconfig`를 생성해야한다.\n",
    "`IBuilderConfig` 객체를 만들 때는 정수 타입을 지원하지 않는 레이어를 위해 FP16 타입 변환에 대한 옵션을 추가해야 합니다. 여기서 주의할 점은, `IBuilderConfig`은 각 타입에 대한 인자를 해당 타입 사용 여부를 나타내는 플래그로 사용한다는 것입니다. 따라서, INT8과 FP16 타입을 모두 사용하려면 `create_config`에서 이들 타입 각각에 해당되는 인자를 모두 `True`로 설정해야 합니다.\n",
    "마지막으로, 앞서 만든 `Calibrator` 객체를 인자로 추가하면 `IBuilderConfig` 객체 생성이 완료됩니다.\n",
    "\n",
    "저 같은 경우 다음과 같은 Warning이 발생했습니다.\n",
    "[W] Missing scale and zero-point for tensor ONNXTRT_Broadcast_1182_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
    "[W] Missing scale and zero-point for tensor model/backbones.0/backbones.0.1/Constant_1_output_0_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
    "[W] Missing scale and zero-point for tensor model.transformer.encoder.layers.3.norm1.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
    "[W] Missing scale and zero-point for tensor model.transformer.decoder.layers.0.norm2.weight_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
    "[W] Missing scale and zero-point for tensor model/backbones.0/backbones.0.1_2/Constant_output_0_output, expect fall back to non-int8 implementation for any layer consuming or producing given tensor\n",
    "\n",
    "위 경고중에서 `ONNXTRT_Broadcast_1182_output`는 왜 발생했는지 찾지못했습니다.\n",
    "그리고 다른 경고들은 TensorRT에서 LayerNorm의 양자화를 지원하지 않기때문에 발생하는 경고입니다. 그리고 `model/backbones.0/backbones.0.1_2/Constant_output_0`는 ACT가 `detr`을 사용하는데 Position Encoding을 할 때 일부 레이어에서 양자화를 지원하지 않기때문에 발생하는 경고입니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f41ced9-2d80-4809-941f-63ac8026e2e8",
   "metadata": {},
   "source": [
    "# Engine 성능 비교"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ce766c-9fe1-4bcb-a88e-f9aeae01d1d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "Engine의 성능을 비교하기 위해 다음 세 가지 타입의 Engine을 생성합니다.\n",
    "1. FP32 원본 엔진(No Calibrator)\n",
    "2. INT8 Static Calibration 적용된 엔진\n",
    "3. INT8 Random Calibration 적용된 엔진(Calibration으로 인한 정확도 향상을 알아보기위해)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2766c5-96c2-4bf7-9d36-36043ac10f19",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
